import cv2
import torch
import pymysql
from datetime import datetime
from facenet_pytorch import MTCNN
from emotiefflib.facial_analysis import EmotiEffLibRecognizer, get_model_list
from flask import Flask, Response, render_template, jsonify
from contextlib import closing
from aip import AipSpeech
import os
from openai import OpenAI

app = Flask(__name__)

# åˆå§‹åŒ–æ‘„åƒå¤´
cap = cv2.VideoCapture(0) #å¤šä¸ªæ‘„åƒå¤´å¯ä»¥æ¢ç´¢å¼•ä¸º1,2,3...
if not cap.isOpened():
    print("Error: cannot open camera. Check index or device.")
    exit()

# åˆå§‹åŒ–æ¨¡å‹
device = "cpu"  # æˆ–è€… "cuda" å¦‚æœæœ‰ GPU ç¯å¢ƒ
mtcnn = MTCNN(keep_all=True, post_process=False, min_face_size=40, device=device)
model_name = get_model_list()[0]
model_path = r"your_path\enet_b0_8_best_vgaf" #åç¼€ä¸€å®šä¸è¦æœ‰.onnx
fer = EmotiEffLibRecognizer(engine="onnx", model_name=model_path, device=device)

# MySQLæ•°æ®åº“é…ç½®
MYSQL_CONFIG = {
    'host': 'localhost',
    'user': 'root',
    'password': '1234',
    'database': 'emotion_db',
    'charset': 'utf8mb4',
    'cursorclass': pymysql.cursors.DictCursor
}

# ç™¾åº¦è¯­éŸ³åˆæˆé…ç½®
APP_ID = "118389588"
API_KEY = "dVzhcO7oUuUL276KIvZ2wnsn"
SECRET_KEY = "7DlHOR7Ehcqa9AnqvjWw6fNaLTmHEhIT"
client_baidu = AipSpeech(APP_ID, API_KEY, SECRET_KEY)

# OpenAIé…ç½®
client_openai = OpenAI(
    api_key="sk-c996dcd7166745e9a9a8e143555c4f32",
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
)

# å…¨å±€å˜é‡å­˜å‚¨ç¬¬ä¸€æ¬¡è¯†åˆ«ç»“æœ
first_emotion_results = None
first_result_recorded = False
detected_probs = {}  # ç”¨äºå­˜å‚¨æƒ…ç»ªæ•°æ®

# åˆå§‹åŒ–MySQLæ•°æ®åº“è¡¨
def init_mysql_db():
    try:
        with closing(pymysql.connect(**MYSQL_CONFIG)) as conn:
            with conn.cursor() as cursor:
                cursor.execute('''
                    CREATE TABLE IF NOT EXISTS emotion_records (
                        id INT AUTO_INCREMENT PRIMARY KEY,
                        timestamp DATETIME NOT NULL,
                        emotion_data TEXT NOT NULL,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
                ''')
                conn.commit()
    except pymysql.MySQLError as e:
        print(f"MySQL Error: {e}")
        raise

init_mysql_db()

class EmotionProcessor:
    @staticmethod
    def process_emotion_results(emotion_probs):
        """å¤„ç†åŸå§‹æƒ…ç»ªæ¦‚ç‡æ•°æ®ï¼Œè¿”å›ä¸»è¦æƒ…ç»ª"""
        # è¿‡æ»¤ä½æ¦‚ç‡æƒ…ç»ªï¼ˆå°äº10%ï¼‰
        filtered = {k: v for k, v in emotion_probs.items() if v >= 10}
        if not filtered:
            # å¦‚æœæ²¡æœ‰å¤§äº10%çš„æƒ…ç»ªï¼Œå–å‰ä¸¤ä¸ªæœ€é«˜æ¦‚ç‡çš„
            sorted_emotions = sorted(emotion_probs.items(), key=lambda x: x[1], reverse=True)
            filtered = dict(sorted_emotions[:2])

        # è½¬æ¢ä¸ºä¸­æ–‡æƒ…ç»ªæ ‡ç­¾
        emotion_mapping = {
            "happy": "å¼€å¿ƒ",
            "sad": "æ‚²ä¼¤",
            "angry": "æ„¤æ€’",
            "fear": "ææƒ§",
            "surprise": "æƒŠè®¶",
            "neutral": "å¹³é™",
            "disgust": "åŒæ¶"
        }

        return [{
            "emotion_type": emotion_mapping.get(emotion, emotion),
            "intensity": round(prob / 100, 2)  # è½¬æ¢ä¸º0-1èŒƒå›´
        } for emotion, prob in filtered.items()]

class VehicleStrategyGenerator:
    SYSTEM_PROMPT = """ä½œä¸ºè½¦è½½æ™ºèƒ½æƒ…ç»ªè°ƒèŠ‚ç³»ç»Ÿï¼Œè¯·ä¼˜å…ˆæä¾›è¯­éŸ³äº¤äº’æ–¹æ¡ˆï¼Œå¹¶æ ¹æ®æƒ…ç»ªçŠ¶æ€ç»™å‡ºç¯å¢ƒè°ƒèŠ‚å»ºè®®ï¼š

ã€æƒ…ç»ªçŠ¶æ€ã€‘
ç±»å‹ï¼š{emotion_type}
å¼ºåº¦ï¼š{intensity}/1.0

ã€æ–¹æ¡ˆè¦æ±‚ã€‘
1. è¯­éŸ³äº¤äº’æ–¹æ¡ˆï¼ˆé‡ç‚¹ï¼‰ï¼š
   - 3ç§ä¸åŒé£æ ¼çš„å®‰æŠšè¯æœ¯ï¼ˆæ¯ç§é£æ ¼æä¾›2ä¸ªç‰ˆæœ¬ï¼‰
   - æ¯ä¸ªè¯æœ¯éœ€åŒ…å«å®Œæ•´çš„å¼•å¯¼æµç¨‹ï¼ˆ30-50å­—ï¼‰
   - é£æ ¼åŒ…æ‹¬ï¼šæ¸©å’Œå…³æ€€å‹ã€ä¸“ä¸šæŒ‡å¯¼å‹ã€è½»æ¾å¹½é»˜å‹

2. ç¯å¢ƒè°ƒèŠ‚æ–¹æ¡ˆï¼ˆä»ä»¥ä¸‹é€‰æ‹©è‡³å°‘3é¡¹ï¼‰ï¼š
   - æ™ºèƒ½ç©ºè°ƒï¼šæ¸©åº¦/é£é‡/æ¨¡å¼
   - æ°›å›´ç¯å…‰ï¼šè‰²æ¸©ï¼ˆ2700-6500Kï¼‰/äº®åº¦/åŠ¨æ€æ•ˆæœ
   - åº§æ¤…è®¾ç½®ï¼šæŒ‰æ‘©å¼ºåº¦/é€šé£/åŠ çƒ­
   - è½¦çª—å¼€åˆï¼šå¼€åˆæ¯”ä¾‹/é€šé£æ¨¡å¼
   - é©¾é©¶è¾…åŠ©ï¼šå»ºè®®è½¦é€Ÿ/è·Ÿè½¦è·ç¦»

3. å¤šåª’ä½“æ–¹æ¡ˆï¼š
   - æ¨è2é¦–é€‚é…å½“å‰æƒ…ç»ªçš„éŸ³ä¹ï¼ˆæ³¨æ˜æµæ´¾å’ŒèŠ‚å¥ï¼‰
   - éŸ³é¢‘å†…å®¹å»ºè®®ï¼ˆå†¥æƒ³æŒ‡å¯¼/æœ‰å£°ä¹¦ç­‰ï¼‰

ã€è¾“å‡ºæ ¼å¼ã€‘
### è¯­éŸ³äº¤äº’æ–¹æ¡ˆ
[é£æ ¼ç±»å‹] 
1. "å®Œæ•´è¯æœ¯å†…å®¹..."ï¼ˆåŒ…å«å‘¼å¸å¼•å¯¼ã€ç¯å¢ƒå˜åŒ–è¯´æ˜ã€åç»­å»ºè®®ï¼‰
2. "æ›¿ä»£è¯æœ¯å†…å®¹..."

### ç¯å¢ƒè°ƒèŠ‚æ–¹æ¡ˆ
1. [è®¾å¤‡åç§°] å…·ä½“å‚æ•°è®¾ç½®...

### å¤šåª’ä½“æ–¹æ¡ˆ
- éŸ³ä¹æ¨èï¼šã€Šæ›²åã€‹- æ­Œæ‰‹ï¼ˆé£æ ¼ï¼ŒBPMï¼‰"""

    @staticmethod
    def generate_prompt(emotion_data):
        # å¯¹æ¯ç§æƒ…ç»ªç”Ÿæˆå•ç‹¬çš„æç¤ºè¯
        prompts = []
        for emotion in emotion_data:
            prompts.append(VehicleStrategyGenerator.SYSTEM_PROMPT.format(
                emotion_type=emotion['emotion_type'],
                intensity=emotion['intensity']
            ))
        return "\n\n".join(prompts)

    @staticmethod
    def get_feedback_strategy(prompt: str) -> str:
        try:
            # ä½¿ç”¨æµå¼API
            completion = client_openai.chat.completions.create(
                model="qwen-omni-turbo",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯è½¦è½½è¯­éŸ³äº¤äº’è®¾è®¡ä¸“å®¶"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=1000,
                stream=True  # å…³é”®ä¿®æ”¹ï¼šå¯ç”¨æµå¼ä¼ è¾“
            )

            # å¤„ç†æµå¼å“åº”
            full_response = ""
            for chunk in completion:
                if chunk.choices[0].delta.content:
                    full_response += chunk.choices[0].delta.content

            return full_response

        except Exception as e:
            print(f"ç”Ÿæˆç­–ç•¥æ—¶å‡ºé”™: {e}")
            return """### è¯­éŸ³äº¤äº’æ–¹æ¡ˆ
[ç³»ç»Ÿé»˜è®¤] 
1. "æ£€æµ‹åˆ°æ‚¨å½“å‰çŠ¶æ€éœ€è¦æ”¾æ¾ï¼Œå»ºè®®æ·±å‘¼å¸ä¸‰æ¬¡ï¼šæ…¢æ…¢å¸æ°”4ç§’ï¼Œå±æ¯2ç§’ï¼Œç¼“ç¼“å‘¼æ°”6ç§’ï¼Œæˆ‘å·²å°†ç©ºè°ƒè°ƒæ•´ä¸º22â„ƒé€šé£æ¨¡å¼ï¼Œæ˜¯å¦éœ€è¦ä¸ºæ‚¨æ’­æ”¾èˆ’ç¼“éŸ³ä¹ï¼Ÿ"
2. "æ‚¨çš„å®‰å…¨æ˜¯æˆ‘ä»¬çš„é¦–è¦ä»»åŠ¡ï¼Œå·²å¯åŠ¨åº§æ¤…æŒ‰æ‘©åŠŸèƒ½ï¼ˆè…°éƒ¨èšç„¦æ¨¡å¼ï¼‰ï¼Œå‰æ–¹3å…¬é‡Œæœ‰æœåŠ¡åŒºï¼Œå»ºè®®ç¨ä½œä¼‘æ¯"

### ç¯å¢ƒè°ƒèŠ‚æ–¹æ¡ˆ
1. [ç©ºè°ƒ] 22â„ƒè‡ªåŠ¨æ¨¡å¼
2. [åº§æ¤…] å¼€å¯è…°éƒ¨æŒ‰æ‘©ï¼ˆå¼ºåº¦2çº§ï¼‰
3. [å¤šåª’ä½“] æ’­æ”¾è‡ªç„¶ç™½å™ªéŸ³"""

class VehicleOutputFormatter:
    @staticmethod
    def format_strategy(raw_strategy: str) -> str:
        border = "=" * 40
        return f"\n{border}\nğŸš˜ æ™ºèƒ½åº§èˆ±è°ƒèŠ‚æ–¹æ¡ˆï¼ˆè¯­éŸ³ä¼˜å…ˆï¼‰\n{border}\n{raw_strategy}\n{border}"

def generate_vehicle_strategy(emotion_probs):
    """ç”Ÿæˆè½¦è½½è°ƒèŠ‚ç­–ç•¥"""
    emotions_data = EmotionProcessor.process_emotion_results(emotion_probs)
    prompt = VehicleStrategyGenerator.generate_prompt(emotions_data)
    strategy = VehicleStrategyGenerator.get_feedback_strategy(prompt)

    # æå–è¯­éŸ³å†…å®¹ï¼ˆåˆ é™¤å¼€å¤´çš„æƒ…ç»ªè¯†åˆ«ç»“æœï¼‰
    voice_lines = []
    lines = strategy.split('\n')
    for line in lines:
        if line.strip().startswith('1. "') or line.strip().startswith('2. "'):
            voice_lines.append(line.strip()[4:].strip('"'))

    voice_text = " ".join(voice_lines)  # ç›´æ¥è¿æ¥è¯­éŸ³å†…å®¹ï¼Œä¸æ·»åŠ æƒ…ç»ªè¯†åˆ«ç»“æœ

    return {
        "strategy": strategy,
        "voice_text": voice_text
    }

def store_emotion_data(emotion_probs):
    """å­˜å‚¨æƒ…ç»ªæ•°æ®åˆ°MySQLæ•°æ®åº“å¹¶ç”Ÿæˆè¯­éŸ³"""
    global first_emotion_results, first_result_recorded

    if not first_result_recorded:
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        emotion_str = ", ".join([f"{k}: {v:.2f}%" for k, v in emotion_probs.items()])

        try:
            with closing(pymysql.connect(**MYSQL_CONFIG)) as conn:
                with conn.cursor() as cursor:
                    cursor.execute(
                        "INSERT INTO emotion_records (timestamp, emotion_data) VALUES (%s, %s)",
                        (timestamp, emotion_str)
                    )
                    conn.commit()

                    first_emotion_results = emotion_probs
                    first_result_recorded = True

                    print("\né¦–æ¬¡è¯†åˆ«çš„å¤šæƒ…ç»ªç»“æœ:")
                    for emotion, prob in emotion_probs.items():
                        print(f"{emotion}: {prob:.2f}%")

                    # ç”Ÿæˆè°ƒèŠ‚ç­–ç•¥
                    strategy_data = generate_vehicle_strategy(emotion_probs)
                    print("\nç”Ÿæˆçš„è°ƒèŠ‚ç­–ç•¥:")
                    print(VehicleOutputFormatter.format_strategy(strategy_data["strategy"]))

                    # è¯­éŸ³åˆæˆï¼ˆä¿å­˜åˆ°æŒ‡å®šè·¯å¾„ï¼‰
                    text_to_speech(strategy_data["voice_text"],
                                   "D:/PycharmProjects/Visual_project/text/voice/MyVoice.mp3")

        except pymysql.MySQLError as e:
            print(f"å­˜å‚¨æ•°æ®æ—¶å‡ºé”™: {e}")

def text_to_speech(text, file_path="D:/PycharmProjects/Visual_project/text/voice/MyVoice.mp3"):
    """å°†æ–‡æœ¬è½¬æ¢ä¸ºè¯­éŸ³å¹¶ä¿å­˜ä¸ºMP3æ–‡ä»¶"""
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(file_path), exist_ok=True)

    # è°ƒç”¨ç™¾åº¦è¯­éŸ³åˆæˆAPI
    result = client_baidu.synthesis(text, 'zh', 1, {'vol': 5, 'per': 4})  # per=4æ˜¯æƒ…æ„Ÿå¥³å£°

    # ä¿å­˜è¯­éŸ³æ–‡ä»¶
    if not isinstance(result, dict):
        with open(file_path, 'wb') as f:
            f.write(result)
        print(f"\nè¯­éŸ³æ–‡ä»¶å·²ä¿å­˜åˆ°: {file_path}")
    else:
        print("è¯­éŸ³åˆæˆå¤±è´¥:", result)

def gen_frames():
    """ç”Ÿæˆå¤„ç†åçš„è§†é¢‘å¸§"""
    global detected_probs
    while True:
        success, frame_bgr = cap.read()
        if not success:
            break

        frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)
        bboxes, probs = mtcnn.detect(frame_rgb)

        if bboxes is not None and probs is not None:
            for idx, box in enumerate(bboxes):
                if probs[idx] < 0.9:
                    continue
                x1, y1, x2, y2 = box.astype(int)

                # é˜²è¶Šç•Œå¤„ç†
                h, w, _ = frame_rgb.shape
                x1, y1 = max(0, x1), max(0, y1)
                x2, y2 = min(w, x2), min(h, y2)

                face_rgb = frame_rgb[y1:y2, x1:x2, :]
                if face_rgb.size == 0:
                    continue

                # æƒ…ç»ªé¢„æµ‹
                emotions, scores = fer.predict_emotions([face_rgb], logits=True)
                emotion_top = emotions[0]
                scores_tensor = torch.from_numpy(scores)
                probs_tensor = torch.softmax(scores_tensor, dim=1)
                probs_array = probs_tensor[0].numpy()

                # æ›´æ–°æƒ…ç»ªæ•°æ®
                detected_probs = {
                    fer.idx_to_emotion_class[i]: prob * 100
                    for i, prob in enumerate(probs_array)
                }

                # å­˜å‚¨ç¬¬ä¸€æ¬¡è¯†åˆ«ç»“æœ
                if not first_result_recorded:
                    store_emotion_data(detected_probs)

                # å¯è§†åŒ–
                cv2.rectangle(frame_bgr, (x1, y1), (x2, y2), (0, 255, 0), 2)
                cv2.putText(frame_bgr, emotion_top, (x1, y1 - 10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)

                # æ˜¾ç¤ºæ¦‚ç‡åˆ†å¸ƒ
                line_height = 20
                for e_i, prob_val in enumerate(probs_array):
                    emotion_name = fer.idx_to_emotion_class[e_i]
                    text_str = f"{emotion_name}: {prob_val * 100:.1f}%"
                    text_y = y2 + 20 + e_i * line_height
                    cv2.putText(frame_bgr, text_str, (x1, text_y),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)

        ret, buffer = cv2.imencode('.jpg', frame_bgr)
        yield (b'--frame\r\n'
               b'Content-Type: image/jpeg\r\n\r\n' + buffer.tobytes() + b'\r\n')

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/video_feed')
def video_feed():
    return Response(gen_frames(),
                    mimetype='multipart/x-mixed-replace; boundary=frame')

@app.route('/emotion_probs')
def emotion_probs():
    # è¿”å›æƒ…ç»ªæ•°æ®
    if detected_probs:
        return jsonify(detected_probs)
    return jsonify({"status": "No emotion data available"})

@app.route('/get_first_result')
def get_first_result():
    if first_emotion_results:
        return jsonify({
            "status": "success",
            "emotions": first_emotion_results
        })
    return jsonify({"status": "pending"})

if __name__ == '__main__':
    try:
        app.run(host='0.0.0.0', port=5000, debug=True)
    finally:
        cap.release()
        cv2.destroyAllWindows()
